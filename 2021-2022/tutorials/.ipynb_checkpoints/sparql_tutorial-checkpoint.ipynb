{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access and query remote data \n",
    "\n",
    " 1. SPARQL tutorial\n",
    " 2. Execute SPARQL queries with RDFLib on a local RDF file\n",
    " 3. Query a SPARQL endpoint with RDFLib and SPARQLWrapper libraries\n",
    " \n",
    "## 1. SPARQL tutorial\n",
    "\n",
    "Let's see a few useful SPARQL queries that can be performed over **any** RDF dataset. We apply the queries to ARTchives dataset, so that while we learn SPARQL we also get to know more the contents of our case study.\n",
    "\n",
    "To see the queries at work, open in a tab of your browser the URL of the [ARTchives SPARQL endpoint](http://artchives.fondazionezeri.unibo.it/sparql), copy and paste the queries in the text area, and press the play button (top-right): \n",
    "\n",
    "### 1.1. Get all the classes URIs\n",
    "\n",
    "This query helps us to understand what are the main entities described in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_query = \"\"\"\n",
    "SELECT DISTINCT ?class_uri\n",
    "WHERE {\n",
    "    ?anything a ?class_uri .  \n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query returns a list of 3 URIs.\n",
    "\n",
    " 1. http://www.wikidata.org/entity/Q31855\n",
    " 2.\thttp://www.wikidata.org/entity/Q5\n",
    " 3.\thttp://www.wikidata.org/entity/Q9388534\n",
    "\n",
    "\n",
    "### 1.2. Get all the classes labels\n",
    "\n",
    "However, the URIs are not very intuitive. Let's get their names. Classes names are usually recorded as values of the property `rdfs:label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_and_labels_query = \"\"\"\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT DISTINCT ?class_uri ?class_name\n",
    "WHERE {\n",
    "    ?anything a ?class_uri .\n",
    "    ?class_uri rdfs:label ?class_name .\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query returns zero results. How come?\n",
    "\n",
    "In ARTchives we find concepts (classes) and relations (predicates) taken from the Wikidata vocabulary. However, the labels and the definitions of these concepts are not stored in ARTchives!\n",
    "\n",
    "We will see later how to get this information while integrating ARTchives and Wikidata. For the time being, let's put the label as an optional value to be returned by our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_and_opt_labels_query = \"\"\"\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT DISTINCT ?class_uri ?class_name\n",
    "WHERE {\n",
    "    ?anything a ?class_uri .\n",
    "    OPTIONAL {?class_uri rdfs:label ?class_name .}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query returns the same list of three URIs and an empty column for the variable `class_name`.\n",
    "\n",
    "### 1.3 Individuals belonging to classes\n",
    "\n",
    "Let's get some individuals that fall under those classes to better understand. ARTchives is a rather small dataset, so visualising all the individuals does not take much effort for the SPARQL endpoint. However, usually datasets are far bigger and querying for *all the individuals* might get the endpoint to go in **timeout**, hence not answering the query. \n",
    "\n",
    "Since our objective is to get some insights, we can limit the results to 100 results. We can also order our results by class, so that it's easier to understand what is what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_and_individuals_100_query = \"\"\"\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT DISTINCT ?class_uri ?individual ?individual_label\n",
    "WHERE {\n",
    "    ?individual a ?class_uri .\n",
    "    ?individual rdfs:label ?individual_label .\n",
    "} ORDER BY ?class_uri\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** *LIMIT* does not allow you to specify that you want a sample of all classes.\n",
    "\n",
    "As you see from results, entities represent art historians (Q5), collections (Q9388534), and keepers (Q31855).\n",
    "\n",
    "Some URIs are repeated, that's because multiple labels for the individuals were (erroneously) recorded (e.g. \"Getty Research institute\" and \"  Getty research institute\"). To show only one value among the possible values of `rdfs:label`, we tune the query variable `?individual_label`, asking for a **sample** of values and **grouping results by** variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_and_individuals_100_query = \"\"\"\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "SELECT DISTINCT ?class_uri ?individual (sample(?individual_label) as ?label)\n",
    "WHERE {\n",
    "    ?individual a ?class_uri .\n",
    "    ?individual rdfs:label ?individual_label .\n",
    "} \n",
    "GROUP BY ?label ?individual ?class_uri\n",
    "ORDER BY ?class_uri ?label\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** When grouping results by a variable, you must specify **ALL** the query variables in order of priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Get all the properties having a certain class as domain\n",
    "\n",
    "So far we still don't know precisely which information *can be* recorded for the individuals of the dataset, i.e. we don't know which predicates (also called properties) apply to instances of a certain class.\n",
    "\n",
    "Let's select individuals of all the classes and look for all the possible properties recorded for individuals of that class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_by_class_query = \"\"\"\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "SELECT DISTINCT ?class_uri ?property\n",
    "WHERE {\n",
    "    ?individual rdf:type ?class_uri ; ## rdf:type == a\n",
    "                ?property ?value .\n",
    "}\n",
    "ORDER BY ?class_uri ?property\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB** The fact that a predicate *can apply* to that class, does not imply that there is *always* a value for that <subject> <predicate>. For instance, despite an art historian *may* be described with it's biography, it does not mean that the biography it's available for every historians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Count the number of individuals in a class\n",
    "\n",
    "Now, let's explore closely what is in ARTchives, for instance counting the number of individuals belonging to a certain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_individuals_by_class_query = \"\"\"\n",
    "\n",
    "SELECT ?class (COUNT(?individual) AS ?tot)\n",
    "WHERE { ?individual a ?class .}\n",
    "GROUP BY ?class ?tot\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we have 6 keepers, 25 art historians and 26 collections.\n",
    "\n",
    "### 1.6 Count the number of occurrences of properties\n",
    "\n",
    "Let's see to what extent properties are represented in the dataset, e.g. let's see how many collections have historical notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_count_by_class_query = \"\"\"\n",
    "SELECT ?class ?property (COUNT(?property) AS ?prop_count)\n",
    "WHERE { ?individual a ?class ; ?property ?something .}\n",
    "GROUP BY ?class ?property ?prop_count \n",
    "ORDER BY ?class DESC(?prop_count)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Select only certain values for a variable\n",
    "\n",
    "Let's say we don't care about properties related to keepers and we want to select only certain classes. We can specify in the query a list of `VALUES` for the variable `?class` that we want to be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_count_class_values_query = \"\"\"\n",
    "SELECT ?class ?property (COUNT(?property) AS ?prop_count)\n",
    "WHERE { \n",
    "    VALUES ?class {<http://www.wikidata.org/entity/Q5> <http://www.wikidata.org/entity/Q9388534>} # look at the syntax\n",
    "    ?individual a ?class ; ?property ?something .\n",
    " }\n",
    "GROUP BY ?class ?property ?prop_count \n",
    "ORDER BY ?class DESC(?prop_count)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operator applies to any other cases. For instance:\n",
    "\n",
    " * select only individuals that are related to `1921` - > `VALUES ?something {'1921'}` \n",
    " * select only birthplaces -> `VALUES ?property {<http://www.wikidata.org/direct/prop/P19>}` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Let's play with the SPARQL enpoint interface\n",
    "\n",
    "There are several GUIs (Graphical User Interface) that can be built on top of a SPARQL endpoint. Generally, these offer a number of operations that you can perform on results. The most important ones are the following:\n",
    "\n",
    " * display data in several formats (tabular, json raw response, charts)\n",
    " * download data (usually in a default JSON format or CSV).\n",
    " \n",
    "**Note for future yourself. What format to choose for download?** It depends. If you are going to  use the  data locally CSV is fine. If you plan to directly use the result data for visualization purposes, JSON *might* be better, but it really depends on which library you will use (some may require you to pass a JSON object as input, some others prefer a table). So think about it ahead!\n",
    "\n",
    "### 1.9 Analyse the raw response of a SPARQL query\n",
    "\n",
    "Let's have a look at the JSON result of the query called `count_individuals_by_class_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\"\"\n",
    "{\n",
    "  \"head\" : {\n",
    "    \"vars\" : [ \"class\", \"tot\" ]\n",
    "  },\n",
    "  \"results\" : {\n",
    "    \"bindings\" : [ {\n",
    "      \"class\" : {\n",
    "        \"type\" : \"uri\",\n",
    "        \"value\" : \"http://www.wikidata.org/entity/Q31855\"\n",
    "      },\n",
    "      \"tot\" : {\n",
    "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
    "        \"type\" : \"literal\",\n",
    "        \"value\" : \"6\"\n",
    "      }\n",
    "    }, {\n",
    "      \"class\" : {\n",
    "        \"type\" : \"uri\",\n",
    "        \"value\" : \"http://www.wikidata.org/entity/Q5\"\n",
    "      },\n",
    "      \"tot\" : {\n",
    "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
    "        \"type\" : \"literal\",\n",
    "        \"value\" : \"25\"\n",
    "      }\n",
    "    }, {\n",
    "      \"class\" : {\n",
    "        \"type\" : \"uri\",\n",
    "        \"value\" : \"http://www.wikidata.org/entity/Q9388534\"\n",
    "      },\n",
    "      \"tot\" : {\n",
    "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
    "        \"type\" : \"literal\",\n",
    "        \"value\" : \"26\"\n",
    "      }\n",
    "    } ]\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON returned by **any SPARQL query** has always the same structure, namely: a dictionary with two keys: `head` and `results`.\n",
    "\n",
    "<pre>\n",
    "{\n",
    "  <span style=\"color:blue\">\"head\"</span> : {\n",
    "    \"vars\" : [ \"class\", \"tot\" ]\n",
    "  },\n",
    "  <span style=\"color:blue\">\"results\"</span> : {\n",
    "    \"bindings\" : [ {\n",
    "      \"class\" : {\n",
    "...\n",
    "</pre>\n",
    "\n",
    "**HEADINGS** the value of the key `head` is a dictionary with a key called `vars`, whose value is a list including all the query variables. In our case the query variables are `[\"class\", \"tot\"]`, which correspond to the names of columns in the tabular view of results\n",
    " \n",
    "<pre>\n",
    "{\n",
    "  <b>\"head\"</b> : {\n",
    "    <span style=\"color:blue\">\"vars\" : [ \"class\", \"tot\" ]</span>\n",
    "  },\n",
    "  \"results\" : {\n",
    "    \"bindings\" : [ {\n",
    "      \"class\" : {\n",
    "...\n",
    "</pre>\n",
    "\n",
    "**RESULTS** the value of the key `results` is a dictionary with a key called `bindings`, whose value is a list of dictionaries. \n",
    "\n",
    "<pre>\n",
    "{\n",
    "  \"head\" : {\n",
    "    \"vars\" : [ \"class\", \"tot\" ]</span>\n",
    "  },\n",
    "  <b>\"results\"</b> : {\n",
    "    <span style=\"color:blue\">\"bindings\"</span> : [ \n",
    "        {...},\n",
    "        {...},\n",
    "        {...}\n",
    "    ]\n",
    "  }\n",
    "}    \n",
    "</pre>\n",
    "\n",
    "**ROW** Each dictionary in the list corresponds to a row shown in the tabular results.\n",
    "\n",
    "**MAPPING ROW/COLUMN** Each dictionary/row includes as many dictionaries as the number of query variables (in our case: class and tot). The keys in the dictionary/row are the names of the column/query varaiables.\n",
    "\n",
    "<pre>\n",
    "{\n",
    "  \"head\" : {\n",
    "    \"vars\" : [ \"class\", \"tot\" ]</span>\n",
    "  },\n",
    "  <b>\"results\"</b> : {\n",
    "    \"bindings\" : [ \n",
    "        <span style=\"color:blue\">{\n",
    "            \"class\": {...},\n",
    "            \"tot\": {...}\n",
    "        }</span>,\n",
    "        {...},\n",
    "        {...}\n",
    "    ]\n",
    "  }\n",
    "}    \n",
    "</pre>\n",
    "\n",
    "**CELL** The value of the key/column (class, tot) corresponds to a cell of the tabular result. For every cell two/three variables are recorded according to the type of value, namely:\n",
    "\n",
    " * the type, that can be either `uri` or `literal`\n",
    "\n",
    " <pre>\n",
    "      \"class\" : {\n",
    "        <span style=\"color:blue\">\"type\" : \"uri\",</span>\n",
    "        \"value\" : \"http://www.wikidata.org/entity/Q31855\"\n",
    "      },\n",
    "      \"tot\" : {\n",
    "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
    "        <span style=\"color:blue\">\"type\" : \"literal\",</span>\n",
    "        \"value\" : \"6\"\n",
    "      }\n",
    " </pre>\n",
    "\n",
    " * the actual `value` (either the http URI or the string)\n",
    " \n",
    " <pre>\n",
    "      \"class\" : {\n",
    "        \"type\" : \"uri\",\n",
    "        <span style=\"color:blue\">\"value\" : \"http://www.wikidata.org/entity/Q31855\"</span>\n",
    "      },\n",
    "      \"tot\" : {\n",
    "        \"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",\n",
    "        \"type\" : \"literal\",\n",
    "        <span style=\"color:blue\">\"value\" : \"6\"</span>\n",
    "      }\n",
    " </pre>\n",
    " \n",
    " * eventually, if specified in the data, the `datatype` of the literal:\n",
    " \n",
    " <pre>\n",
    "      \"class\" : {\n",
    "        \"type\" : \"uri\",\n",
    "        \"value\" : \"http://www.wikidata.org/entity/Q31855\"\n",
    "      },\n",
    "      \"tot\" : {\n",
    "        <span style=\"color:blue\">\"datatype\" : \"http://www.w3.org/2001/XMLSchema#integer\",</span>\n",
    "        \"type\" : \"literal\",\n",
    "        \"value\" : \"6\"\n",
    "      }\n",
    "</pre>\n",
    "\n",
    "\n",
    "### 1.10 Download and query the JSON result with python\n",
    "\n",
    "I've downloaded the results of the query in the folder `resources`, called `sparql_query_result.json`.\n",
    "\n",
    "To query SPARQL JSON results I can use python JSON built-in library (in the end, it's a dictionary, no need of special libraries for this).\n",
    "\n",
    "For instance, let's open the file and transform the JSON in a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'head': {'vars': ['class', 'tot']},\n",
      " 'results': {'bindings': [{'class': {'type': 'uri',\n",
      "                                     'value': 'http://www.wikidata.org/entity/Q31855'},\n",
      "                           'tot': {'datatype': 'http://www.w3.org/2001/XMLSchema#integer',\n",
      "                                   'type': 'literal',\n",
      "                                   'value': '6'}},\n",
      "                          {'class': {'type': 'uri',\n",
      "                                     'value': 'http://www.wikidata.org/entity/Q5'},\n",
      "                           'tot': {'datatype': 'http://www.w3.org/2001/XMLSchema#integer',\n",
      "                                   'type': 'literal',\n",
      "                                   'value': '25'}},\n",
      "                          {'class': {'type': 'uri',\n",
      "                                     'value': 'http://www.wikidata.org/entity/Q9388534'},\n",
      "                           'tot': {'datatype': 'http://www.w3.org/2001/XMLSchema#integer',\n",
      "                                   'type': 'literal',\n",
      "                                   'value': '26'}}]}}\n"
     ]
    }
   ],
   "source": [
    "import json , pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1) # just to pretty print results\n",
    "\n",
    "with open('../resources/sparql_query_result.json','r') as results:\n",
    "    data = json.load(results)  \n",
    "    pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over the JSON and print out colum names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "tot\n"
     ]
    }
   ],
   "source": [
    "for column in data[\"head\"][\"vars\"]: # enter the list\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over the results and print the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The class http://www.wikidata.org/entity/Q31855 has 6 individuals\n",
      "The class http://www.wikidata.org/entity/Q5 has 25 individuals\n",
      "The class http://www.wikidata.org/entity/Q9388534 has 26 individuals\n"
     ]
    }
   ],
   "source": [
    "for result in data[\"results\"][\"bindings\"]:  # enter the list of dictionaries // do you remember \"for row in rows\"?\n",
    "    res_class = result['class']['value']    # the value of the cell under column \"class\"\n",
    "    res_tot = result['tot']['value']        # the value of the cell under column \"tot\"\n",
    "    print('The class', res_class,'has', res_tot, 'individuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute SPARQL queries with RDFLib on a local RDF file\n",
    "\n",
    "Along with the methods provided by RDFlib for iterating over triples, we can perform SPARQL queries on our local data (again, via RDFLib). To do that we need:\n",
    "\n",
    " * parse a local RDF file (we have it! see `../resources/artchives.nq`) into a RDFLib graph\n",
    " * write a SPARQL query as a string variable (we have plenty already) \n",
    " * iterate over results of the query (how are the data organised?)\n",
    " \n",
    "The result of a SPARQL query performed over a SPARQL endpoint returns a JSON object organised as we saw. When you query a local RDFLib graph, the result is a **list of tuples**. Every tuple include as many values as the number of query variables (in our case these are 2, `class` and `tot`), which are served in the same order as they appear in the `SELECT` clause. For instance, a `SELECT ?class ?tot` clause returns tuples like `(\"http://www.wiki...\" , \"6\")`.\n",
    "Values of the tuples can be accessed by position (e.g. `query_res[0]`) or by variable name (e.g. `query_res[\"class\"]`)\n",
    "\n",
    "Let's print for each row the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q9388534 25\n",
      "http://www.wikidata.org/entity/Q31855 5\n",
      "http://www.wikidata.org/entity/Q5 24\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "\n",
    "# create an empty Graph\n",
    "g = rdflib.ConjunctiveGraph()\n",
    "\n",
    "# parse a local RDF file by specifying the format\n",
    "result = g.parse(\"../resources/artchives.nq\", format='nquads')\n",
    "\n",
    "query_results = g.query(\n",
    "    \"\"\"SELECT ?class (COUNT(?individual) AS ?tot)\n",
    "    WHERE { ?individual a ?class .}\n",
    "    GROUP BY ?class ?tot\"\"\")\n",
    "\n",
    "for query_res in query_results:\n",
    "    print(query_res[0], query_res[\"tot\"]) # notice the two alternative ways to recall values in the tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query a SPARQL endpoint with RDFLib and SPARQLWrapper libraries\n",
    "\n",
    "Querying data via SPARQL endpoint interfaces requires you to **separate the code** for collecting the data (the SPARQL query that you perform against the endpoint) from the code for manipulating results. This is not always convenient for **reproducibility** reasons (what if you forget where you saved the SPARQL query and you need to perform it again?), and it is highly discouraged when you need to show your results to a broader community (they want everything in one place runnable with one command line). Moreover, in many cases, to **dump data** locally (whether these are query results or the entire dataset) and parse them in a RDFLib graph it's not possible or convenient, e.g. dumping the entire Wikidata graph would require you ~60GB storage (only for the zipped file!). Moreover, while the online data keep being updated, the local copy goes easily **out-to-date**.\n",
    "\n",
    "RDFLib allows you to **query remote SPARQL endpoints** and to get up-to-date result data in the same JSON format you'd get if you dump result data from the interface (See above section 1.9). In order to query remote endpoints we use both RDFLib and an extended version of it, called `SPARQLWrapper`.\n",
    "\n",
    "In order to query a remote SPARQL endpoint you'll need:\n",
    "\n",
    " * If you have mac *you may need* to tweak the certificates (use an unverified one) for querying an external service (import ssl and copy/paste the line below) \n",
    " * get the URL of the API of the SPARQL endpoint (sometimes it corresponds to the URL of the interface for querying via SPARQL, sometimes the URL is different!)\n",
    " * prepare the SPARQL query (this means you need also to study how data are organised in the source to be queried! try the query on the GUI of the SPARQL endpoint - if available - to see if the query you wrote is correct)\n",
    " * create the wrapper around the SPARQL API (via SPARQLWrapper library)\n",
    " * send the query and get the JSON results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://wikiba.se/ontology#Dump http://creativecommons.org/ns#license http://creativecommons.org/publicdomain/zero/1.0/\n",
      "http://wikiba.se/ontology#Dump http://schema.org/softwareVersion 1.0.0\n",
      "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2020-03-02T23:00:02Z\n",
      "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2020-03-02T23:24:15Z\n",
      "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2020-03-02T23:24:16Z\n",
      "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2020-03-02T23:24:19Z\n",
      "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2020-03-02T23:24:20Z\n",
      "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2020-03-02T23:24:24Z\n",
      "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2020-03-02T23:51:40Z\n",
      "http://wikiba.se/ontology#Dump http://schema.org/dateModified 2020-03-02T23:51:41Z\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# get the endpoint API\n",
    "wikidata_endpoint = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql\"\n",
    "\n",
    "# prepare the query : 10 random triples\n",
    "my_SPARQL_query = \"\"\"\n",
    "SELECT *\n",
    "WHERE {?s ?p ?o}\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# set the endpoint \n",
    "sparql_wd = SPARQLWrapper(wikidata_endpoint)\n",
    "# set the query\n",
    "sparql_wd.setQuery(my_SPARQL_query)\n",
    "# set the returned format\n",
    "sparql_wd.setReturnFormat(JSON)\n",
    "# get the results\n",
    "results = sparql_wd.query().convert()\n",
    "\n",
    "# manipulate the result\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"s\"][\"value\"], result[\"p\"][\"value\"], result[\"o\"][\"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Integrate art historians' birth places from Wikidata\n",
    "\n",
    "In the last tutorial we saw how to add triples about historians' birthplaces (`wdt:P19`). We were able to add this information by looking in wikidata for the URI of the place of a certain historian and by adding the new triple manually to the graph. Now we can do this operation **systematically**, meaning that:\n",
    "\n",
    "*For every art historian in ARTchives that corresponds to an individual available in Wikidata, we get their birthplaces and we add this information to our graph*\n",
    "\n",
    "To do that we need to:\n",
    "\n",
    " * get the list of historians in ARTchives that are also available in Wikidata\n",
    " * prepare a SPARQL query that returns the birthplace \n",
    " * for each of them, send a query to Wikidata to get the birthplace\n",
    " * if there is a result, we add a triple to our graph\n",
    " \n",
    "**GET THE HISTORIANS' URIs** We need to take into account a couple of caveats.\n",
    " \n",
    " * How to distinguish historians from other people that are mentioned in ARTchives? We use the pattern `?collection wdt.P170(has creator) ?creator`, which is the only mandatory predicate that distinguishes historians from other people.\n",
    " * How do we select only the historians that are both in ARTchives and Wikidata? We match a substring in the URI (if it includes the substring \"wikidata.org/entity/\", we are sure this is a Wikidata entity.\n",
    "\n",
    "So:\n",
    "\n",
    " * We first iterate over the triples in the graph to get the URIs of the historians. \n",
    " * Then we transform these URIs (that RDFLIb considers as `RDFLib.URIRef`) to strings. We know that we will use this list of URIs in a SPARQL query, possibly in the `VALUES` operator, therefore we wrap the strings in the hooks `<>` \n",
    " * and we add these URIs to a set (a list of unique values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<http://www.wikidata.org/entity/Q41616785>', '<http://www.wikidata.org/entity/Q18935222>', '<http://www.wikidata.org/entity/Q3051533>', '<http://www.wikidata.org/entity/Q60185>', '<http://www.wikidata.org/entity/Q55453618>', '<http://www.wikidata.org/entity/Q2824734>', '<http://www.wikidata.org/entity/Q1296486>', '<http://www.wikidata.org/entity/Q1089074>', '<http://www.wikidata.org/entity/Q85761254>', '<http://www.wikidata.org/entity/Q1641821>', '<http://www.wikidata.org/entity/Q1271052>', '<http://www.wikidata.org/entity/Q88907>', '<http://www.wikidata.org/entity/Q19997512>', '<http://www.wikidata.org/entity/Q1712683>', '<http://www.wikidata.org/entity/Q457739>', '<http://www.wikidata.org/entity/Q1715096>', '<http://www.wikidata.org/entity/Q61913691>', '<http://www.wikidata.org/entity/Q537874>', '<http://www.wikidata.org/entity/Q1373290>', '<http://www.wikidata.org/entity/Q90407>', '<http://www.wikidata.org/entity/Q6700132>', '<http://www.wikidata.org/entity/Q995470>', '<http://www.wikidata.org/entity/Q1629748>', '<http://www.wikidata.org/entity/Q3057287>'}\n"
     ]
    }
   ],
   "source": [
    "# import all we need\n",
    "from rdflib import Namespace , Literal , URIRef\n",
    "from rdflib.namespace import RDF , RDFS\n",
    "\n",
    "# bind the uncommon namespaces\n",
    "wd = Namespace(\"http://www.wikidata.org/entity/\") # remember that a prefix matches a URI until the last slash (or hashtag #)\n",
    "wdt = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
    "art = Namespace(\"https://w3id.org/artchives/\")\n",
    "\n",
    "# Get the list of art historians in our graph \"g\"\n",
    "arthistorians_list = set()\n",
    "\n",
    "# iterate over the triples in the graph\n",
    "for s,p,o in g.triples(( None, wdt.P170, None)):   # people \"o\" are the creator \"wdt.P170\" of a collection \"s\"\n",
    "    if \"wikidata.org/entity/\" in str(o):           # look for the substring to filter wikidata entities only\n",
    "        arthistorians_list.add('<' + str(o) + '>')     # remember to transform them in strings! \n",
    "    \n",
    "print(arthistorians_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUERY THE ENDPOINT** Now that we have the list of historians we have two options:\n",
    " * **MMMNEE..** for each of them, we prepare a query to be sent to Wikidata. However, this implies sending many small queries to an external service that may have (reasonably) imposed a query limit (If you ever get an error `429: Too Many requests`, see [here](https://stackoverflow.com/questions/62396801/how-to-handle-too-many-requests-on-wikidata-using-sparqlwrapper) the reason.)\n",
    " * **BETTER** we send only one (bigger) query to the Wikidata endpoint where all the historians are terms specified in a `VALUES` list. The result table of our query will include for every row (1) the URI of the historian, (2) the URI of the birthplace, and (3) the label of the birth place *in english only!* (be aware that Wikidata has labels for every language!)\n",
    " \n",
    "**INTEGRATE THE DATA INTO THE GRAPH** Once the wrapper and the query are set we manipulate results:\n",
    " \n",
    " * only if the birthplace is found we look also for its label \n",
    " * only for those birthplaces that have both URI and label we create a new triple to be added to our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "historian: http://www.wikidata.org/entity/Q18935222\n",
      "found: http://www.wikidata.org/entity/Q64 Berlin\n",
      "historian: http://www.wikidata.org/entity/Q18935222\n",
      "found: http://www.wikidata.org/entity/Q64 Berlin\n",
      "historian: http://www.wikidata.org/entity/Q18935222\n",
      "found: http://www.wikidata.org/entity/Q64 Berlin\n",
      "historian: http://www.wikidata.org/entity/Q1629748\n",
      "found: http://www.wikidata.org/entity/Q64 Berlin\n",
      "historian: http://www.wikidata.org/entity/Q1629748\n",
      "found: http://www.wikidata.org/entity/Q64 Berlin\n",
      "historian: http://www.wikidata.org/entity/Q1629748\n",
      "found: http://www.wikidata.org/entity/Q64 Berlin\n",
      "historian: http://www.wikidata.org/entity/Q537874\n",
      "found: http://www.wikidata.org/entity/Q84 London\n",
      "historian: http://www.wikidata.org/entity/Q537874\n",
      "found: http://www.wikidata.org/entity/Q84 London\n",
      "historian: http://www.wikidata.org/entity/Q537874\n",
      "found: http://www.wikidata.org/entity/Q84 London\n",
      "historian: http://www.wikidata.org/entity/Q1089074\n",
      "found: http://www.wikidata.org/entity/Q220 Rome\n",
      "historian: http://www.wikidata.org/entity/Q6700132\n",
      "found: http://www.wikidata.org/entity/Q220 Rome\n",
      "historian: http://www.wikidata.org/entity/Q2824734\n",
      "found: http://www.wikidata.org/entity/Q279 Modena\n",
      "historian: http://www.wikidata.org/entity/Q1373290\n",
      "found: http://www.wikidata.org/entity/Q495 Turin\n",
      "historian: http://www.wikidata.org/entity/Q1373290\n",
      "found: http://www.wikidata.org/entity/Q495 Turin\n",
      "historian: http://www.wikidata.org/entity/Q1373290\n",
      "found: http://www.wikidata.org/entity/Q495 Turin\n",
      "historian: http://www.wikidata.org/entity/Q457739\n",
      "found: http://www.wikidata.org/entity/Q649 Moscow\n",
      "historian: http://www.wikidata.org/entity/Q457739\n",
      "found: http://www.wikidata.org/entity/Q649 Moscow\n",
      "historian: http://www.wikidata.org/entity/Q457739\n",
      "found: http://www.wikidata.org/entity/Q649 Moscow\n",
      "historian: http://www.wikidata.org/entity/Q90407\n",
      "found: http://www.wikidata.org/entity/Q3075 Fürth\n",
      "historian: http://www.wikidata.org/entity/Q90407\n",
      "found: http://www.wikidata.org/entity/Q3075 Fürth\n",
      "historian: http://www.wikidata.org/entity/Q90407\n",
      "found: http://www.wikidata.org/entity/Q3075 Fürth\n",
      "historian: http://www.wikidata.org/entity/Q88907\n",
      "found: http://www.wikidata.org/entity/Q1726 Munich\n",
      "historian: http://www.wikidata.org/entity/Q88907\n",
      "found: http://www.wikidata.org/entity/Q1726 Munich\n",
      "historian: http://www.wikidata.org/entity/Q1296486\n",
      "found: http://www.wikidata.org/entity/Q715 Heilbronn\n",
      "historian: http://www.wikidata.org/entity/Q1296486\n",
      "found: http://www.wikidata.org/entity/Q715 Heilbronn\n",
      "historian: http://www.wikidata.org/entity/Q1296486\n",
      "found: http://www.wikidata.org/entity/Q715 Heilbronn\n",
      "historian: http://www.wikidata.org/entity/Q1641821\n",
      "found: http://www.wikidata.org/entity/Q3949 Unna\n",
      "historian: http://www.wikidata.org/entity/Q1641821\n",
      "found: http://www.wikidata.org/entity/Q3949 Unna\n",
      "historian: http://www.wikidata.org/entity/Q1641821\n",
      "found: http://www.wikidata.org/entity/Q3949 Unna\n",
      "historian: http://www.wikidata.org/entity/Q60185\n",
      "found: http://www.wikidata.org/entity/Q1055 Hamburg\n",
      "historian: http://www.wikidata.org/entity/Q60185\n",
      "found: http://www.wikidata.org/entity/Q1055 Hamburg\n",
      "historian: http://www.wikidata.org/entity/Q60185\n",
      "found: http://www.wikidata.org/entity/Q1055 Hamburg\n",
      "historian: http://www.wikidata.org/entity/Q995470\n",
      "found: http://www.wikidata.org/entity/Q242478 Levoča\n",
      "historian: http://www.wikidata.org/entity/Q995470\n",
      "found: http://www.wikidata.org/entity/Q242478 Levoča\n",
      "historian: http://www.wikidata.org/entity/Q995470\n",
      "found: http://www.wikidata.org/entity/Q242478 Levoča\n",
      "historian: http://www.wikidata.org/entity/Q88907\n",
      "found: http://www.wikidata.org/entity/Q1726 Munich\n",
      "historian: http://www.wikidata.org/entity/Q3057287\n",
      "found: http://www.wikidata.org/entity/Q671011 Jördenstorf\n",
      "historian: http://www.wikidata.org/entity/Q3057287\n",
      "found: http://www.wikidata.org/entity/Q671011 Jördenstorf\n",
      "historian: http://www.wikidata.org/entity/Q3057287\n",
      "found: http://www.wikidata.org/entity/Q671011 Jördenstorf\n",
      "historian: http://www.wikidata.org/entity/Q1715096\n",
      "found: http://www.wikidata.org/entity/Q525409 Staßfurt\n",
      "historian: http://www.wikidata.org/entity/Q41616785\n",
      "found: http://www.wikidata.org/entity/Q1971847 Nauheim\n",
      "historian: http://www.wikidata.org/entity/Q3051533\n",
      "found: http://www.wikidata.org/entity/Q993164 Epsom\n",
      "historian: http://www.wikidata.org/entity/Q3051533\n",
      "found: http://www.wikidata.org/entity/Q993164 Epsom\n",
      "historian: http://www.wikidata.org/entity/Q3051533\n",
      "found: http://www.wikidata.org/entity/Q993164 Epsom\n",
      "historian: http://www.wikidata.org/entity/Q55453618\n",
      "found: http://www.wikidata.org/entity/Q13367 Forlì\n",
      "historian: http://www.wikidata.org/entity/Q55453618\n",
      "found: http://www.wikidata.org/entity/Q13367 Forlì\n",
      "historian: http://www.wikidata.org/entity/Q55453618\n",
      "found: http://www.wikidata.org/entity/Q13367 Forlì\n",
      "historian: http://www.wikidata.org/entity/Q1712683\n",
      "found: http://www.wikidata.org/entity/Q23008 Mosbach\n",
      "historian: http://www.wikidata.org/entity/Q1712683\n",
      "found: http://www.wikidata.org/entity/Q23008 Mosbach\n",
      "historian: http://www.wikidata.org/entity/Q1712683\n",
      "found: http://www.wikidata.org/entity/Q23008 Mosbach\n"
     ]
    }
   ],
   "source": [
    "# prepare the values to be queried\n",
    "historians = ' '.join(arthistorians_list) # <uri1> <uri2> <uri3> ... <uriN>\n",
    "\n",
    "# prepare the query\n",
    "birthplace_query = \"\"\"\n",
    "PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "SELECT DISTINCT ?historian ?birthplace ?birthplace_label\n",
    "WHERE {\n",
    "    VALUES ?historian {\"\"\"+historians+\"\"\"} . # look how we include a variable in a query string!\n",
    "    ?historian wdt:P19 ?birthplace . \n",
    "    ?birthplace rdfs:label ?birthplace_label .\n",
    "    FILTER (langMatches(lang(?birthplace_label), \"EN\"))\n",
    "    } \n",
    "\"\"\"\n",
    "\n",
    "# set the endpoint \n",
    "sparql_wd = SPARQLWrapper(wikidata_endpoint)\n",
    "# set the query\n",
    "sparql_wd.setQuery(birthplace_query)\n",
    "# set the returned format\n",
    "sparql_wd.setReturnFormat(JSON)\n",
    "# get the results\n",
    "results = sparql_wd.query().convert()\n",
    "\n",
    "# manipulate the result\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    historian_uri = result[\"historian\"][\"value\"]\n",
    "    print(\"historian:\", historian_uri)\n",
    "    if \"birthplace\" in result: # some historians may have no birthplace recorded in Wikidata!\n",
    "        birthplace = result[\"birthplace\"][\"value\"]\n",
    "        if \"birthplace_label\" in result: \n",
    "            birthplace_label = result[\"birthplace_label\"][\"value\"]\n",
    "            print(\"found:\", birthplace, birthplace_label)\n",
    "            \n",
    "            # only if both uri and label are found we add them to the graph\n",
    "            g.add(( URIRef(historian_uri) , URIRef(wdt.P19) , URIRef(birthplace) ))\n",
    "            g.add(( URIRef(birthplace) , RDFS.label , Literal(birthplace_label) ))\n",
    "    else:\n",
    "        print(\"nothing found in wikidata :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STORE** Now that we have added all these new triples to our in-memory graph, we can store these data into a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize(destination='../resources/artchives_birthplaces.nq', format='nquads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the future yourself\n",
    "\n",
    "For the sake of the project you have to:\n",
    " \n",
    " * query and manipulate artchives locally (use the `artchives.nq` file) via RDFlib methods or local SPARQL queries\n",
    " * query external sources remotely (Wikidata and others) using SPARQLWrapper\n",
    " * save the data extracted from external sources along with artchives data locally (create a new file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "Some SPARQL tutorials:\n",
    "  \n",
    " * [SPARQL Tutorial - Apache](https://jena.apache.org/tutorials/sparql.html)\n",
    " * [SPARQL tutorial - stardog](https://www.stardog.com/tutorials/sparql/)\n",
    " * [SPARQL tutorial - Programming historian](https://programminghistorian.org/en/lessons/retired/graph-databases-and-SPARQL)\n",
    " * [SPARQL tutorial - Wikidata](https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial) - very useful for your project! It teaches you how to query data on Wikidata \n",
    " \n",
    "SPARQL and RDFLib:\n",
    "   \n",
    " * [RDFLib documentation on SPARQL](https://rdflib.readthedocs.io/en/stable/intro_to_sparql.html)\n",
    " \n",
    "SPARQLWrapper:\n",
    " \n",
    " * [SPARQLWrapper documentation](https://sparqlwrapper.readthedocs.io/en/latest/main.html)\n",
    " \n",
    "Wikidata resources:\n",
    " * [index of categories](https://www.wikidata.org/wiki/Category:Wikidata:SPARQL_query_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
